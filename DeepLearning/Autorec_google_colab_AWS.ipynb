{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzmsh3P9ylGi",
        "outputId": "6c0612d9-cb97-4e30-8ba3-170cef0b673d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAZHdE4hyzlY",
        "outputId": "e864074f-1c3f-4645-9757-0d14aa78294b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD2CUf2IytBu",
        "outputId": "852cbd3e-5463-4f81-aa48-ac937e1cd7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully.\n",
            "Top 5 popular products: [('B000FS05VG', 1), ('B0002ZW5UQ', 2), ('B000ME2YWG', 3), ('B000KK53L6', 4), ('B000052ZTY', 5)]\n",
            "Product Name: Island Essence Lotion\n",
            "Product ID: B00012U9GC\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class AmazonReview:\n",
        "    productID_to_name = {}\n",
        "    name_to_productID = {}\n",
        "\n",
        "    # Tập này sẽ được xử lý như sau: file Beaty.csv gốc sẽ được lọc ra các cột cần thiết (userId, productId, score, time)\n",
        "    ratingsPath = '/home/haphuthinh/Workplace/School_project/do-an-1/Recommender-system-UIT/AmazonRatingData/Beauty-rating.csv'\n",
        "\n",
        "    # Tập này sẽ được xử lý như sau: file Beaty.csv gốc sẽ được lọc ra các cột cần thiết (productId, title)\n",
        "    productsPath = '/home/haphuthinh/Workplace/School_project/do-an-1/Recommender-system-UIT/AmazonRatingData/Beauty-product.csv'\n",
        "\n",
        "    def loadAmazonReviewDataset(self):\n",
        "        # Định dạng reader cho tập ratings\n",
        "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
        "\n",
        "        # Tải dữ liệu ratings vào Surprise Dataset\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        # Đọc file products để xây dựng ánh xạ productID ↔ title\n",
        "        self.productID_to_name = {}\n",
        "        self.name_to_productID = {}\n",
        "        with open(self.productsPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            productReader = csv.reader(csvfile)\n",
        "            next(productReader)  # Bỏ qua header\n",
        "            for row in productReader:\n",
        "                productID = row[0]\n",
        "                productName = row[1]\n",
        "                self.productID_to_name[productID] = productName\n",
        "                self.name_to_productID[productName] = productID\n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        # Tính độ phổ biến của sản phẩm từ tập ratings\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='', encoding='utf-8') as csvfile:\n",
        "            reviewReader = csv.reader(csvfile)\n",
        "            next(reviewReader)\n",
        "            for row in reviewReader:\n",
        "                productID = row[1]\n",
        "                ratings[productID] += 1\n",
        "        rank = 1\n",
        "        for productID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[productID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "\n",
        "    def getProductName(self, productID):\n",
        "        # Lấy tên sản phẩm từ productID\n",
        "        return self.productID_to_name.get(productID, \"\")\n",
        "\n",
        "    def getProductID(self, productName):\n",
        "        # Lấy productID từ tên sản phẩm\n",
        "        return self.name_to_productID.get(productName, 0)\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    amazonReview = AmazonReview()\n",
        "    dataset = amazonReview.loadAmazonReviewDataset()\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"Top 5 popular products:\", list(amazonReview.getPopularityRanks().items())[:5])\n",
        "\n",
        "    # Lấy tên sản phẩm từ ID\n",
        "    product_name = amazonReview.getProductName(\"B00012U9F8\")\n",
        "    print(f\"Product Name: {product_name}\")\n",
        "\n",
        "    # Lấy ID từ tên sản phẩm\n",
        "    product_id = amazonReview.getProductID(\"Island Essence Lotion\")\n",
        "    print(f\"Product ID: {product_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1k6gbujyyyXY"
      },
      "outputs": [],
      "source": [
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    @staticmethod\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def GetTopN(predictions, n=10, minimumRating=4.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "        for userID, productID, actualRating, estimatedRating, _ in predictions:\n",
        "            if estimatedRating >= minimumRating:\n",
        "                topN[userID].append((productID, estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[userID] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    @staticmethod\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutProductID = leftOut[1]\n",
        "            hit = any(leftOutProductID == productID for productID, _ in topNPredicted[userID])\n",
        "            if hit:\n",
        "                hits += 1\n",
        "            total += 1\n",
        "\n",
        "        return hits / total\n",
        "\n",
        "    @staticmethod\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID, leftOutProductID, actualRating, _, _ in leftOutPredictions:\n",
        "            if actualRating >= ratingCutoff:\n",
        "                hit = any(leftOutProductID == productID for productID, _ in topNPredicted[userID])\n",
        "                if hit:\n",
        "                    hits += 1\n",
        "                total += 1\n",
        "\n",
        "        return hits / total\n",
        "\n",
        "    @staticmethod\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID, leftOutProductID, _, _, _ in leftOutPredictions:\n",
        "            rank = 0\n",
        "            for idx, (productID, _) in enumerate(topNPredicted[userID]):\n",
        "                if productID == leftOutProductID:\n",
        "                    rank = idx + 1\n",
        "                    break\n",
        "            if rank > 0:\n",
        "                summation += 1.0 / rank\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    @staticmethod\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for product1, product2 in pairs:\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(product1[0])\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(product2[0])\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        return (1 - total / n) if n > 0 else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID in topNPredicted.keys():\n",
        "            for productID, _ in topNPredicted[userID]:\n",
        "                rank = rankings[productID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "\n",
        "        return total / n if n > 0 else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            # Iterate over the recommended products for each user\n",
        "            for productID, predictedRating in topNPredicted[userID]:\n",
        "                if predictedRating >= ratingThreshold:\n",
        "                    hit = True\n",
        "                    break\n",
        "            if hit:\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N_JHneTLy1yx"
      },
      "outputs": [],
      "source": [
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    @staticmethod\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def GetTopN(predictions, n=10, minimumRating=4.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "        for userID, productID, actualRating, estimatedRating, _ in predictions:\n",
        "            if estimatedRating >= minimumRating:\n",
        "                topN[userID].append((productID, estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[userID] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    @staticmethod\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutProductID = leftOut[1]\n",
        "            hit = any(leftOutProductID == productID for productID, _ in topNPredicted[userID])\n",
        "            if hit:\n",
        "                hits += 1\n",
        "            total += 1\n",
        "\n",
        "        return hits / total\n",
        "\n",
        "    @staticmethod\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID, leftOutProductID, actualRating, _, _ in leftOutPredictions:\n",
        "            if actualRating >= ratingCutoff:\n",
        "                hit = any(leftOutProductID == productID for productID, _ in topNPredicted[userID])\n",
        "                if hit:\n",
        "                    hits += 1\n",
        "                total += 1\n",
        "\n",
        "        return hits / total\n",
        "\n",
        "    @staticmethod\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID, leftOutProductID, _, _, _ in leftOutPredictions:\n",
        "            rank = 0\n",
        "            for idx, (productID, _) in enumerate(topNPredicted[userID]):\n",
        "                if productID == leftOutProductID:\n",
        "                    rank = idx + 1\n",
        "                    break\n",
        "            if rank > 0:\n",
        "                summation += 1.0 / rank\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    @staticmethod\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for product1, product2 in pairs:\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(product1[0])\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(product2[0])\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        return (1 - total / n) if n > 0 else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "\n",
        "        for userID in topNPredicted.keys():\n",
        "            for productID, _ in topNPredicted[userID]:\n",
        "                rank = rankings[productID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "\n",
        "        return total / n if n > 0 else 0\n",
        "\n",
        "    @staticmethod\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            # Iterate over the recommended products for each user\n",
        "            for productID, predictedRating in topNPredicted[userID]:\n",
        "                if predictedRating >= ratingThreshold:\n",
        "                    hit = True\n",
        "                    break\n",
        "            if hit:\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2qQfbItLy4Y6"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split, LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "import gc\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "\n",
        "class EvaluationData:\n",
        "    def __init__(self, dataset, popularityRankings):\n",
        "        print(\"Creating an evaluation data...\")\n",
        "        self.rankings = popularityRankings\n",
        "\n",
        "        # Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = dataset.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "\n",
        "        print(\"Number of users in the full trainset:\", self.fullTrainSet.n_users)\n",
        "        print(\"Number of items in the full trainset:\", self.fullTrainSet.n_items)\n",
        "\n",
        "        print(\"Full trainset: \", self.fullTrainSet)\n",
        "\n",
        "        # Build a 75/25 train/test split for measuring accuracy\n",
        "        print(\"Building train set and test set...\")\n",
        "        self.trainSet, self.testSet = train_test_split(dataset, test_size=.25, random_state=1)\n",
        "\n",
        "        # Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        print(\"Building LOOCV train set and test set...\")\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(dataset):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "\n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "\n",
        "        # Compute similarity matrix between items for measuring diversity\n",
        "        print(\"Building item similarity matrix...\")\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "\n",
        "\n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "\n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "\n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                         i in trainset.all_items() if i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "\n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "\n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "\n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "\n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "\n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "\n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X-3gC71ey5zf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EvaluatedAlgorithm:\n",
        "\n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "\n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "\n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())\n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a movie the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)\n",
        "            # See how often we recommended a movie the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "\n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted,\n",
        "                                                                   evaluationData.GetFullTrainSet().n_users,\n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "\n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted,\n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "\n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "\n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jIJxAZsXy8X0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Evaluator:\n",
        "    algorithms = []\n",
        "\n",
        "    def __init__(self, dataset, rankings, amazonReview):\n",
        "        # Khởi tạo dataset và ánh xạ productID ↔ title\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "        self.amazonReview = amazonReview  # Thêm đối tượng amazonReview để lấy tên sản phẩm từ productID\n",
        "\n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "\n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # In kết quả\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if doTopN:\n",
        "            print(\"{:<15} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for name, metrics in results.items():\n",
        "                print(\"{:<15} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                    name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                    metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<15} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for name, metrics in results.items():\n",
        "                print(\"{:<15} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "\n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if doTopN:\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\")\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "\n",
        "    def SampleTopNRecs(self, testSubject, k=10):\n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "\n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "\n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "\n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "\n",
        "            recommendations = []\n",
        "\n",
        "            print(\"\\nWe recommend:\")\n",
        "            for userID, productID, actualRating, estimatedRating, _ in predictions:\n",
        "                recommendations.append((productID, estimatedRating))\n",
        "\n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for productID, rating in recommendations[:k]:\n",
        "                # Sử dụng AmazonReview để lấy tên sản phẩm từ ID\n",
        "                productName = self.amazonReview.getProductName(productID)\n",
        "                if productName:\n",
        "                    print(f\"{productName}: {rating:.2f}\")\n",
        "                else:\n",
        "                    print(f\"Unknown Product (ID: {productID}): {rating:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class AutoRec(object):\n",
        "\n",
        "    def __init__(self, visibleDimensions, epochs=200, hiddenDimensions=50, learningRate=0.1, batchSize=100):\n",
        "\n",
        "        self.visibleDimensions = visibleDimensions\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDimensions = hiddenDimensions\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "        self.optimizer = tf.keras.optimizers.RMSprop(self.learningRate)\n",
        "        \n",
        "                \n",
        "    def Train(self, X):\n",
        "        \n",
        "        self.initialize_weights_biases()\n",
        "        for epoch in range(self.epochs):\n",
        "            for i in range(0, X.shape[0], self.batchSize):\n",
        "                epochX = X[i:i+self.batchSize]\n",
        "                self.run_optimization(epochX)\n",
        "\n",
        "\n",
        "            print(\"Trained epoch \", epoch)\n",
        "\n",
        "    def GetRecommendations(self, inputUser):\n",
        "                \n",
        "        # Feed through a single user and return predictions from the output layer.\n",
        "        rec = self.neural_net(inputUser)\n",
        "        \n",
        "        # It is being used as the return type is Eager Tensor.\n",
        "        return rec[0]\n",
        "    \n",
        "    def initialize_weights_biases(self):\n",
        "        # Create varaibles for weights for the encoding (visible->hidden) and decoding (hidden->output) stages, randomly initialized\n",
        "        self.weights = {\n",
        "            'h1': tf.Variable(tf.random.normal([self.visibleDimensions, self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.hiddenDimensions, self.visibleDimensions]))\n",
        "            }\n",
        "        \n",
        "        # Create biases\n",
        "        self.biases = {\n",
        "            'b1': tf.Variable(tf.random.normal([self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.visibleDimensions]))\n",
        "            }\n",
        "    \n",
        "    def neural_net(self, inputUser):\n",
        "\n",
        "        #tf.set_random_seed(0)\n",
        "        \n",
        "        # Initialization of weights and biases was moved out to the initialize_weights_biases function above\n",
        "        # This lets us avoid resetting them on every batch of training, which was a bug in earlier versions of\n",
        "        # this script.\n",
        "        \n",
        "        # Create the input layer\n",
        "        self.inputLayer = inputUser\n",
        "        \n",
        "        # hidden layer\n",
        "        hidden = tf.nn.sigmoid(tf.add(tf.matmul(self.inputLayer, self.weights['h1']), self.biases['b1']))\n",
        "        \n",
        "        # output layer for our predictions.\n",
        "        self.outputLayer = tf.nn.sigmoid(tf.add(tf.matmul(hidden, self.weights['out']), self.biases['out']))\n",
        "        \n",
        "        return self.outputLayer\n",
        "    \n",
        "    def run_optimization(self, inputUser):\n",
        "        with tf.GradientTape() as g:\n",
        "            pred = self.neural_net(inputUser)\n",
        "            loss = tf.keras.losses.MSE(inputUser, pred)\n",
        "            \n",
        "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
        "        \n",
        "        gradients = g.gradient(loss, trainable_variables)\n",
        "        \n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from surprise import AlgoBase\n",
        "from surprise import PredictionImpossible\n",
        "import numpy as np\n",
        "from AutoRec import AutoRec\n",
        "\n",
        "class AutoRecAlgorithm(AlgoBase):\n",
        "\n",
        "    def __init__(self, epochs=100, hiddenDim=100, learningRate=0.01, batchSize=100, sim_options={}):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDim = hiddenDim\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        numUsers = trainset.n_users\n",
        "        numItems = trainset.n_items\n",
        "        \n",
        "        trainingMatrix = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "        \n",
        "        for (uid, iid, rating) in trainset.all_ratings():\n",
        "            trainingMatrix[int(uid), int(iid)] = rating / 5.0\n",
        "        \n",
        "        # Create an RBM with (num items * rating values) visible nodes\n",
        "        autoRec = AutoRec(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n",
        "        autoRec.Train(trainingMatrix)\n",
        "\n",
        "        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "        \n",
        "        for uiid in range(trainset.n_users):\n",
        "            if (uiid % 50 == 0):\n",
        "                print(\"Processing user \", uiid)\n",
        "            recs = autoRec.GetRecommendations([trainingMatrix[uiid]])\n",
        "            \n",
        "            for itemID, rec in enumerate(recs):\n",
        "                self.predictedRatings[uiid, itemID] = rec * 5.0\n",
        "        \n",
        "        return self\n",
        "\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unkown.')\n",
        "        \n",
        "        rating = self.predictedRatings[u, i]\n",
        "        \n",
        "        if (rating < 0.001):\n",
        "            raise PredictionImpossible('No valid prediction exists.')\n",
        "            \n",
        "        return rating\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from AutoRecAlgorithm import AutoRecAlgorithm\n",
        "from surprise import NormalPredictor\n",
        "from Evaluator import Evaluator\n",
        "from AmazonRating import AmazonReview\n",
        "import gc\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def LoadAmazonReviewData():\n",
        "    amazonReview = AmazonReview()\n",
        "    print(\"Loading Amazon ratings...\")\n",
        "    # Load dataset\n",
        "    data = amazonReview.loadAmazonReviewDataset()\n",
        "    print(\"\\nComputing product popularity ranks so we can measure novelty later...\")\n",
        "    # Get product popularity ranks\n",
        "    rankings = amazonReview.getPopularityRanks()\n",
        "    return (amazonReview, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(amazonReview, evaluationData, rankings) = LoadAmazonReviewData()\n",
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings, amazonReview)\n",
        "\n",
        "#Autoencoder\n",
        "AutoRec = AutoRecAlgorithm()\n",
        "evaluator.AddAlgorithm(AutoRec, \"AutoRec\")\n",
        "\n",
        "# Fight!\n",
        "evaluator.Evaluate(True)\n",
        "\n",
        "\n",
        "# Clean up memory\n",
        "gc.collect()\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "##########################################\n",
        "evaluator.SampleTopNRecs(\"A3PB71Q63XF43G\")\n",
        "##########################################\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate total time\n",
        "total_time = end_time - start_time\n",
        "print(f\"Total time for recommending top N: {total_time} seconds\")\n",
        "\n",
        "# Get the process ID\n",
        "pid = os.getpid()\n",
        "\n",
        "# Get the process\n",
        "process = psutil.Process(pid)\n",
        "\n",
        "# Get the memory info\n",
        "memory_info = process.memory_info()\n",
        "\n",
        "# Print the memory usage\n",
        "print(f\"Memory usage: {memory_info.rss / (1024 * 1024)} MB\")\n",
        "\n",
        "# Clean up memory\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
